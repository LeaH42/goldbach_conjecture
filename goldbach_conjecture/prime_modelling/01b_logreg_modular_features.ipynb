{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637a57b2-cbd6-4384-8bac-a66f0e91ff13",
   "metadata": {},
   "source": [
    "### Prove of concept: we should be able to build an almost perfect prime model on modular features \n",
    "**modular feature**: is a number dividable by a given prime <br>\n",
    "target whether int is/ isnt prime is almost a simple linear combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf84b8-2f74-47b4-9c1a-ca2a8aae67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274287b6-531b-4438-a5d1-d6ff79f470d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "prime_lim = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3247c89-0cd7-4faa-a74d-763fd7201bdf",
   "metadata": {},
   "source": [
    "#### Build Core Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc926c9-baa4-4ce6-ad32-497a1b6b11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in prime numbers\n",
    "primes = np.load(f'../../artifacts/primes/prime_{prime_lim}.npy')\n",
    "primes[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311335e-df2c-453d-93d4-8ee2cd6a97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to natural numbers with binary target\n",
    "natural_numbers = np.arange(0,prime_lim)\n",
    "target = np.zeros(prime_lim, dtype=bool)\n",
    "target[primes] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ea8ec-5517-42c6-a9fa-1c0734978ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data={'n': natural_numbers[2:], 'y': target[2:]})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30ace8-8152-4e3c-b249-4c51b4894438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### A: try out small data sets with modular features\n",
    "--> if we can actually almost garantie a modular signal per prime in training, the models should show almost perfect performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843951f1-8247-4b84-b054-d5baab47b10b",
   "metadata": {},
   "source": [
    "models: \n",
    "- prime cutoff 10000, and the lower 100 primes are used for features --> converges\n",
    "- prime cutoff 100000, and the lower 100 primes are used for features --> converges\n",
    "- prime cutoff 500000, and the lower 100 primes are used for features --> converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7cd76-5770-4d3f-8fc4-0cf989e70ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    10000: {},\n",
    "    100000: {},\n",
    "    prime_lim: {},\n",
    "} # prime_cutoff as key for models\n",
    "\n",
    "n_modular_features = 100 # not all features\n",
    "target_col = 'y'\n",
    "\n",
    "\n",
    "for prime_cutoff in model_dict.keys():\n",
    "    print(prime_cutoff,'\\n')\n",
    "    \n",
    "    data_a = data[data['n']<prime_cutoff].copy()\n",
    "    print(data_a.shape)\n",
    "    \n",
    "    # create modular features\n",
    "    features = [data_a['n'].apply(lambda x: 1 if (x%prime==0 and x!=prime) else 0).values for prime in primes[:n_modular_features]]\n",
    "    features = np.array(features).T\n",
    "    feature_col = [f\"mod_{str(prime)}\" for prime in primes[:n_modular_features]]\n",
    "\n",
    "    data_a = pd.concat([data_a, pd.DataFrame(features, columns=feature_col)], axis=1)\n",
    "\n",
    "    print(data_a.head())\n",
    "\n",
    "    # split in train & test    \n",
    "    X, y = data_a[feature_col], data_a[target_col]\n",
    "    print(target_col in feature_col)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # train logistic regression as start\n",
    "    # lbfgs solver, l2 penalty\n",
    "    clf = LogisticRegressionCV(cv=10, random_state=0, max_iter=500).fit(X_train, y_train)\n",
    "    \n",
    "    # store models and data\n",
    "    model_dict[prime_cutoff]['data'] = data_a.copy()\n",
    "    model_dict[prime_cutoff]['model'] = clf\n",
    "    \n",
    "    model_dict[prime_cutoff]['X_train'] = X_train.copy()\n",
    "    model_dict[prime_cutoff]['X_test'] = X_test.copy()\n",
    "    model_dict[prime_cutoff]['y_train'] = y_train.copy()\n",
    "    model_dict[prime_cutoff]['y_test'] = y_test.copy()\n",
    "\n",
    "    print('Training completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212576c-c446-410e-8678-c8b8c894a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions for evaluation of models\n",
    "\n",
    "for prime_cutoff in model_dict.keys():\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "    curmod['y_pred'] = curmod['model'].predict(curmod['X_test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe6a8a-8926-4c00-839e-51158d3f9030",
   "metadata": {},
   "source": [
    "#### Check overall performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b5ed1-6919-4b3a-ac73-f233df5b6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prime_cutoff in model_dict.keys():\n",
    "    print(f'model with prime cutoff {prime_cutoff}')\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "    print('confusion matrix \\n', confusion_matrix(curmod['y_test'], curmod['y_pred']), '\\n')\n",
    "\n",
    "    print(classification_report(curmod['y_test'], curmod['y_pred']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2642a-bd25-4f1c-840e-f4fb5972b56e",
   "metadata": {},
   "source": [
    "very few misclassification in every model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd40bfd-27ee-4ce3-b5a6-5b892f1f2beb",
   "metadata": {},
   "source": [
    "#### Confusion matrix depending on signal in modular features for models\n",
    "**When do we have misclassifications?** <br>\n",
    "-> all false positives (not prime, but predicted as prime) should have no signal in modular features (like no single modular feature = 1) <br>\n",
    "-> all false negatives (prime, but not predicted as prime) cannot have any modular signal (as they are not prime) -> so how does this misclassification happen? <br>\n",
    "\n",
    "<br> \n",
    "- ideally, the model would perfectly learn that no modular signal = prime -> that would eliminate all false negatives <br>\n",
    "- introducing other features than just modular features would help to reduce the false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f066a-effc-4949-89fe-b3f45d22c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prime_cutoff in enumerate(model_dict.keys()):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(9, 3.5))\n",
    "\n",
    "    curmod = model_dict[prime_cutoff]\n",
    "\n",
    "    mod_features = curmod['model'].feature_names_in_\n",
    "\n",
    "    # add new superposition of modular features to dataframe\n",
    "    curmod['X_test']['any_mod'] = curmod['X_test'][mod_features].aggregate('sum',axis=1)>0\n",
    "\n",
    "    # confusion matrix with any modular features\n",
    "    cm_mod = confusion_matrix(curmod['y_test'][curmod['X_test']['any_mod']], curmod['y_pred'][curmod['X_test']['any_mod']], labels=curmod['model'].classes_)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mod,\n",
    "                              display_labels=curmod['model'].classes_)\n",
    "\n",
    "    disp.plot(ax=ax[0])\n",
    "    \n",
    "    # confusion matrix without any modular features\n",
    "    cm_nonmod = confusion_matrix(curmod['y_test'][curmod['X_test']['any_mod']==False], curmod['y_pred'][curmod['X_test']['any_mod']==False], labels=curmod['model'].classes_)\n",
    "\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_nonmod,\n",
    "                                  display_labels=curmod['model'].classes_)\n",
    "    disp.plot(ax=ax[1])\n",
    "\n",
    "    ax[0].set_title('Modular features = 1', size=10)\n",
    "    ax[1].set_title('Modular features = 0', size=10)\n",
    "\n",
    "    plt.suptitle(f\"Model with prime cutoff {prime_cutoff}\")\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024c3fd-234a-4926-942a-470f516db0ae",
   "metadata": {},
   "source": [
    "- first model has perfect classification\n",
    "- second model classifies some primes with instead of modular features, but no modular feature = prime (and is true in test set)\n",
    "- third model correctly classifies any number with any modular signal as \"not prime\" but misclassifies all which are prime although there is no modular signal\n",
    "  --> we now have to find other features which might help with this false positive group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0cdb8-aec9-46e3-b2a9-3bc2d10a8ea5",
   "metadata": {},
   "source": [
    "### B: lets try to move away from modular features\n",
    "- modular features are trivial, because if we provide them all, the recognition of \"prime / no prime\" is a simple linear superposition <br>\n",
    "- lets try to find other features and reduce modular features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8b8ca-f960-4682-9995-ee3cbe07a3aa",
   "metadata": {},
   "source": [
    "models:\n",
    "- prime cutoff 500000, and the lower 50 primes are used for features, no other features -> converges\n",
    "- prime cutoff 500000, and the lower 50 primes are used for features, some other normalized features added\n",
    "  -> converges\n",
    "  -> do the new features help AT ALL in reducing false positives?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c9d64-8186-4ef7-a8a2-6455b36a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modular_features = 50 # not all features\n",
    "target_col = 'y'\n",
    "\n",
    "data_b = data[data['n']<prime_cutoff].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a50d3f-c3cb-4c90-9028-1e052d9912e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modular features\n",
    "features = [data_b['n'].apply(lambda x: 1 if (x%prime==0 and x!=prime) else 0).values for prime in primes[:n_modular_features]]\n",
    "features = np.array(features).T\n",
    "feature_col = [f\"mod_{str(prime)}\" for prime in primes[:n_modular_features]]\n",
    "\n",
    "data_b = pd.concat([data_b, pd.DataFrame(features, columns=feature_col)], axis=1)\n",
    "\n",
    "# print(data_b.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac61a34-0d39-4017-badc-009ae2b29c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-modular features\n",
    "\n",
    "data_b['n+1'] = data_b['n'].apply(lambda x: x+1)\n",
    "data_b['n-1'] = data_b['n'].apply(lambda x: x-1)\n",
    "data_b['2n'] = data_b['n'].apply(lambda x: x*2)\n",
    "data_b['n**2'] = data_b['n'].apply(lambda x: x**2)\n",
    "data_b['n%2'] = data_b['n'].apply(lambda x: x%2) # this might be too strong as an indicator?\n",
    "\n",
    "# distance to last prime?\n",
    "# number of primes before this number\n",
    "# dividing current number by last prime? \n",
    "# what is last prime?\n",
    "\n",
    "data_b['last_prime']=data_b['n'].apply(lambda x: primes[primes<x].max() if x!=2 else -1)\n",
    "data_b['primes_lower_n']=data_b['n'].apply(lambda x: len(primes[primes<x]) if x!=2 else 0)\n",
    "data_b['n_div_last_prime']=data_b.apply(lambda x: x['n']/x['last_prime'] if x['n']!=2 else -1, axis=1)\n",
    "data_b['n_minus_last_prime']=data_b.apply(lambda x: x['n']-x['last_prime'] if x['n']!=2 else -1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3ed8d-abeb-47ed-911f-0f127853d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = data_b.columns.drop(target_col)\n",
    "print(target_col in feature_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cec54-75b0-4b1d-80ad-06baa84c3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_b[feature_col] = scaler.fit_transform(data_b[feature_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b72292-edf7-4d4f-a3ca-4b1e0c7933bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train & test    \n",
    "X, y = data_b[feature_col], data_b[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train logistic regression as start\n",
    "# lbfgs solver, l2 penalty\n",
    "clf = LogisticRegressionCV(cv=10, random_state=0, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c5383-c5a3-49e2-8de3-f82a9d15e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccffaa-6704-43ec-893a-ad0c7202eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefs != feature importance...\n",
    "cm.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
